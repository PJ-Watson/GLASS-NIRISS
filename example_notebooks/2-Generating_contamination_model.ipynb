{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a contamination model\n",
    "\n",
    "This notebook covers generating a segmentation map, detection catalogue, and contamination model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we setup the directory structure and environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This is currently a necessity; newer pipeline reductions do not work well with grizli\n",
    "os.environ[\"CRDS_CONTEXT\"] = \"jwst_1173.pmap\"\n",
    "\n",
    "root_dir = Path(os.getenv(\"ROOT_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, logging\n",
    "from astropy.io import fits\n",
    "import grizli\n",
    "from grizli import utils, prep, jwst_utils, multifit\n",
    "from grizli.pipeline import auto_script\n",
    "import numpy as np\n",
    "\n",
    "print(\"Grizli version: \", grizli.__version__)\n",
    "\n",
    "# Quiet JWST log warnings\n",
    "jwst_utils.QUIET_LEVEL = logging.WARNING\n",
    "jwst_utils.set_quiet_logging(jwst_utils.QUIET_LEVEL)\n",
    "\n",
    "root_name = \"glass-a2744\"\n",
    "\n",
    "# Set up the grizli directory structure\n",
    "grizli_home_dir = root_dir / \"2024_08_16_A2744_v4\" / \"grizli_home\"\n",
    "(grizli_home_dir / \"Extractions\").mkdir(exist_ok=True)\n",
    "\n",
    "os.chdir(grizli_home_dir / \"Prep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by generating a catalogue with the default parameters, and seeing how that looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiband_catalog_args = auto_script.get_yml_parameters()[\"multiband_catalog_args\"]\n",
    "\n",
    "phot_cat = auto_script.multiband_catalog(field_root=root_name, **multiband_catalog_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show a quick plot of the areas not included in the segmentation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, join\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.visualization as astrovis\n",
    "\n",
    "plot = True\n",
    "\n",
    "prev_seg_map = fits.getdata(f\"{root_name}-ir_seg.fits\").astype(int)\n",
    "\n",
    "detect_img = fits.getdata(f\"{root_name}-ir_drc_sci.fits\")\n",
    "\n",
    "if plot:\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    plot_kwargs = {\n",
    "        \"norm\": astrovis.ImageNormalize(\n",
    "            detect_img,\n",
    "            interval=astrovis.ManualInterval(0, 5),\n",
    "            stretch=astrovis.LogStretch(),\n",
    "        ),\n",
    "        \"origin\": \"lower\",\n",
    "        \"cmap\": \"plasma\",\n",
    "    }\n",
    "    axs[0].imshow(\n",
    "        detect_img,\n",
    "        **plot_kwargs,\n",
    "    )\n",
    "    axs[1].imshow(\n",
    "        detect_img * (1 - (prev_seg_map > 0)),\n",
    "        **plot_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant fraction of the light in the cluster is not included in the segmentation map. This missing light will not be accounted for in the contamination model, and will cause problems for any overlapping objects in the dispersed image. \n",
    "\n",
    "This can be solved by making two changes:\n",
    "\n",
    "* Lowering the detection threshold.\n",
    "* Not subtracting a background from the detection image.\n",
    "\n",
    "The latter is particularly important for cluster observations.\n",
    "\n",
    "Here, we use `photutils` to generate a new segmentation map. The primary purpose here is to include as much of the light as possible; the exact source deblending is a matter of preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "from photutils.segmentation import SegmentationImage, SourceCatalog, SourceFinder\n",
    "\n",
    "detect_img = fits.getdata(f\"{root_name}-ir_drc_sci.fits\")\n",
    "detect_wht = fits.getdata(f\"{root_name}-ir_drc_wht.fits\")\n",
    "\n",
    "err = np.sqrt(1 / detect_wht)\n",
    "\n",
    "pixel_stddev = 3\n",
    "\n",
    "# Smooth the detection image\n",
    "conv_detect = convolve(detect_img, Gaussian2DKernel(pixel_stddev))\n",
    "\n",
    "# Mask any bad pixels\n",
    "mask = (~np.isfinite(detect_img)) | (~np.isfinite(err)) | (~np.isfinite(detect_img))\n",
    "\n",
    "# The minimum contrast ratio between detected peaks when deblending\n",
    "contrast = 1e-5\n",
    "# The number of levels to use when deblending\n",
    "nlevels = 32\n",
    "# The minimum number of connected pixels for a source to be detected\n",
    "npixels = 9\n",
    "# The number of CPU processes to use for deblending\n",
    "nproc = 16\n",
    "# The connectivity for each pixel: 8 includes diagonal connections\n",
    "connectivity = 8\n",
    "\n",
    "finder = SourceFinder(\n",
    "    npixels=npixels,\n",
    "    nlevels=nlevels,\n",
    "    contrast=contrast,\n",
    "    nproc=16,\n",
    "    connectivity=connectivity,\n",
    ")\n",
    "segment_map = finder(\n",
    "    conv_detect,\n",
    "    threshold=1.0 * err, # Use a low detection threshold\n",
    ")\n",
    "\n",
    "new_seg_hdul = fits.HDUList(\n",
    "    hdus=[\n",
    "        fits.PrimaryHDU(\n",
    "            data=segment_map.copy(),\n",
    "            header=fits.getheader(f\"{root_name}-ir_drc_sci.fits\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "new_seg_hdul.writeto(\n",
    "    f\"photutils_seg_map_{nlevels}_{contrast:.0E}_{connectivity}.fits\", overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:**\n",
    "\n",
    "At this point, further source deblending is a matter of personal preference, and will heavily depend on the individual science case. For the GLASS A2744 field, I combined the `photutils` segmentation map with a previous `sep`-derived map manually, as `sep` (or `sep-pjw`) produced more irregular shapes, better matched to the shape of the PSF from foreground stars. For most cases, the `photutils` map is likely to be sufficient.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we regenerate both the detection catalogue, and multiband photometric catalogue, following the `grizli` format for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glass_niriss.pipeline import regen_catalogue\n",
    "\n",
    "# Or whatever name you came up with after the previous step\n",
    "prev_seg_name = f\"{root_name}-ir_seg_mod_3_ordered2.fits\"\n",
    "\n",
    "segment_map = fits.getdata(prev_seg_name)\n",
    "\n",
    "use_regen_seg = np.asarray(segment_map).astype(np.int32)\n",
    "new_cat = regen_catalogue(\n",
    "    use_regen_seg,\n",
    "    root=f\"{root_name}-ir\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cat_name = f\"{root_name}-ir.cat.fits\"\n",
    "\n",
    "multiband_catalog_args = auto_script.get_yml_parameters()[\"multiband_catalog_args\"]\n",
    "multiband_catalog_args[\"run_detection\"] = False\n",
    "multiband_catalog_args[\"filters\"] = [\"f115wn-clear\",\"f150wn-clear\",\"f200wn-clear\"]\n",
    "\n",
    "if not (Path.cwd() / f\"{root_name}_phot.fits\").is_file():\n",
    "    phot_cat = auto_script.multiband_catalog(\n",
    "        field_root=root_name,\n",
    "        master_catalog=prev_cat_name,\n",
    "        **multiband_catalog_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an attempt to use the NGDEEP GRISMCONF files. Results may vary; whilst the wavelength calibration for the first order spectra improved, the overall contamination model was a noticeable regression over the `*221215.conf` files used by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print (dir(grizli))\n",
    "# # print (grizli.GRIZLI_PATH)\n",
    "# # print (dir(grizli.grismconf))\n",
    "\n",
    "# import requests, shutil\n",
    "# from astropy.table import Table\n",
    "# import numpy as np\n",
    "# from scipy.interpolate import CubicSpline\n",
    "# from itertools import product\n",
    "\n",
    "# conf_dir = Path(grizli.GRIZLI_PATH) / \"CONF\"\n",
    "\n",
    "# os.chdir(conf_dir)\n",
    "\n",
    "# calib_name = \"NGDEEP_NIRISS_CALIB-v5\"\n",
    "# orig_conf_dir = conf_dir / f\"orig_{calib_name}\"\n",
    "# grizli_conf_dir = conf_dir / \"wfss-grism-configuration\"\n",
    "\n",
    "# if not orig_conf_dir.is_dir():\n",
    "#     response = requests.get(\n",
    "#         f\"https://zenodo.org/record/10955821/files/npirzkal/{calib_name}.zip?download=1\"\n",
    "#     )\n",
    "\n",
    "#     with open(f\"{calib_name}.zip\", \"wb\") as f:\n",
    "#         f.write(response.content)\n",
    "\n",
    "#     shutil.unpack_archive(f\"{calib_name}.zip\", f\"orig_{calib_name}\")\n",
    "\n",
    "#     for each_file in orig_conf_dir.glob(\"*/*\"):  # grabs all files\n",
    "#         print(each_file)\n",
    "#         each_file.rename(orig_conf_dir / each_file.name)\n",
    "\n",
    "# new_conf_dir = conf_dir / f\"{calib_name}\"\n",
    "\n",
    "# if not new_conf_dir.is_dir():\n",
    "#     new_conf_dir.mkdir()\n",
    "#     for conf_file in orig_conf_dir.glob(\"*.conf\"):\n",
    "#         shutil.copy2(conf_file, new_conf_dir / conf_file.name)\n",
    "#     filts = [\"F115W\", \"F150W\", \"F200W\"]\n",
    "#     pupils = [\"GR150C\", \"GR150R\"]\n",
    "#     orders = [\"-1\", \"+0\", \"+1\", \"+2\", \"+3\"]\n",
    "#     # filt = \"F200W\"\n",
    "#     # pupil = \"GR150C\"\n",
    "#     # order = \"-1\"\n",
    "#     for filt, pupil, order in product(filts, pupils, orders):\n",
    "#         print(filt, pupil, order)\n",
    "#         NP_path = [*orig_conf_dir.glob(f\"*{pupil}*{filt}*{order}*\")][0]\n",
    "#         if order == \"+1\":\n",
    "#             shutil.copy2(NP_path, new_conf_dir / NP_path.name)\n",
    "#             continue\n",
    "#         orig_NP_sens = Table.read(NP_path)\n",
    "#         orig_GB_sens = Table.read(\n",
    "#             [\n",
    "#                 *grizli_conf_dir.glob(\n",
    "#                     f\"NIRISS.{pupil}.{filt}.{order}.*\".replace(\"+\", \"\")\n",
    "#                 )\n",
    "#             ][0]\n",
    "#         )\n",
    "\n",
    "        # print (np.nansum(orig_NP_sens[\"sensitivity\"]))\n",
    "        # print (np.nansum(orig_GB_sens[\"SENSITIVITY\"]))\n",
    "        NP_diff = np.diff(orig_NP_sens[\"wavelength\"])\n",
    "        # print (np.nanmedian(NP_diff), np.nanmax(NP_diff), np.nanmin(NP_diff))\n",
    "        GB_diff = np.diff(orig_GB_sens[\"WAVELENGTH\"])\n",
    "        # print (np.argwhere(orig_GB_sens[\"SENSITIVITY\"]>0))\n",
    "        # print (np.nanmedian(GB_diff), np.nanmax(GB_diff), np.nanmin(GB_diff))\n",
    "\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # interp_GB = np.interp(orig_NP_sens[\"wavelength\"], orig_GB_sens[\"WAVELENGTH\"]/1e4, orig_GB_sens[\"SENSITIVITY\"])\n",
    "        spline_GB = CubicSpline(\n",
    "            orig_GB_sens[\"WAVELENGTH\"] / 1e4, orig_GB_sens[\"SENSITIVITY\"]\n",
    "        )\n",
    "        interp_GB = spline_GB(orig_NP_sens[\"wavelength\"])\n",
    "        flux_scale = np.nansum(interp_GB) / np.nansum(orig_NP_sens[\"sensitivity\"])\n",
    "        print(flux_scale)\n",
    "        # plt.plot(orig_NP_sens[\"wavelength\"], interp_GB)\n",
    "        # plt.plot(orig_NP_sens[\"wavelength\"], orig_NP_sens[\"sensitivity\"])\n",
    "        # plt.plot(orig_NP_sens[\"wavelength\"], orig_NP_sens[\"sensitivity\"]*flux_scale)\n",
    "        # plt.show()\n",
    "        if order == \"+0\" and filt == \"F150W\":\n",
    "            flux_scale *= 2\n",
    "        orig_NP_sens[\"sensitivity\"] *= flux_scale\n",
    "        if order == \"-1\":\n",
    "            orig_NP_sens[\"sensitivity\"] = orig_NP_sens[\"sensitivity\"][::-1]\n",
    "        orig_NP_sens.write(new_conf_dir / NP_path.name)\n",
    "\n",
    "# for each_file in new_conf_dir.glob(\"*\"):  # grabs all files\n",
    "#     print(each_file)\n",
    "#     shutil.copy2(each_file, conf_dir / each_file.name)\n",
    "#     # each_file.rename(orig_conf_dir / each_file.name)\n",
    "\n",
    "# # print ([*orig_conf_dir.glob(f\"*{pupil}*{filt}*{order}*\")])\n",
    "# # print ([*grizli_conf_dir.glob(f\"NIRISS.{pupil}.{filt}.{order}.*\".replace(\"+\",\"\"))])\n",
    "# # print (f\"*{pupil}*{filt}*{order}*\".strip(\"+\"))\n",
    "\n",
    "\n",
    "# # trg_path = src_path.parent # gets the parent of the folder\n",
    "# # each_file.rename(trg_path.joinpath(each_file.name)) # moves to parent folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a new segmentation map and detection catalogue in hand, we now generate the contamination models. There are several parameters that can be changed here; these are just the parameters used for the GLASS catalogue.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Warning:**\n",
    "\n",
    "Depending on the number of objects detected, generating the contamination models **will** require a significant amount of memory. It will also take 10-30 minutes to run. At least you only have to do it once.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = auto_script.get_yml_parameters()\n",
    "\n",
    "# The number of processes to use\n",
    "cpu_count = 8\n",
    "\n",
    "os.chdir(grizli_home_dir / \"Prep\")\n",
    "\n",
    "rate_files = [str(s) for s in Path.cwd().glob(\"*_rate.fits\")][:]\n",
    "grism_files = [str(s) for s in Path.cwd().glob(\"*GrismFLT.fits\")][:]\n",
    "\n",
    "if len(grism_files)==0:\n",
    "\n",
    "    grism_prep_args = kwargs[\"grism_prep_args\"]\n",
    "\n",
    "    # For now, turn off refining contamination model with polynomial fits\n",
    "    grism_prep_args[\"refine_niter\"] = 0\n",
    "\n",
    "    # Flat-flambda spectra\n",
    "    grism_prep_args[\"init_coeffs\"] = [1.0]\n",
    "\n",
    "    grism_prep_args[\"mask_mosaic_edges\"] = False\n",
    "\n",
    "    # Here we use all of the detected objects.\n",
    "    # These can be adjusted based on how deep the spectra/visits are\n",
    "    grism_prep_args[\"refine_mag_limits\"] = [14.0, 50.0]\n",
    "    grism_prep_args[\"prelim_mag_limit\"] = 50.0\n",
    "\n",
    "    # Which filter to use as direct image?  Will try in order of the list until a match is found.\n",
    "    grism_prep_args[\"gris_ref_filters\"] = {\n",
    "        \"GR150R\": [\"F115W\", \"F150W\", \"F200W\"],\n",
    "        \"GR150C\": [\"F115W\", \"F150W\", \"F200W\"],\n",
    "    }\n",
    "\n",
    "    grism_prep_args[\"files\"] = rate_files\n",
    "    grp = auto_script.grism_prep(\n",
    "        field_root=root_name, pad=800, cpu_count=cpu_count, **grism_prep_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try extracting an object to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(grizli_home_dir / \"Extractions\")\n",
    "flt_files = [str(s) for s in Path.cwd().glob(\"*GrismFLT.fits\")][:]\n",
    "\n",
    "grp = multifit.GroupFLT(\n",
    "    grism_files=flt_files,\n",
    "    catalog=f\"{root_name}-ir.cat.fits\",\n",
    "    cpu_count=-1,\n",
    "    sci_extn=1,\n",
    "    pad=800,\n",
    ")\n",
    "\n",
    "pline = {\n",
    "    \"kernel\": \"square\",\n",
    "    \"pixfrac\": 1.0,\n",
    "    \"pixscale\": 0.03,\n",
    "    \"size\": 50,\n",
    "    \"wcs\": None,\n",
    "}\n",
    "args = auto_script.generate_fit_params(\n",
    "    pline=pline,\n",
    "    field_root=root_name,\n",
    "    min_sens=0.0,\n",
    "    min_mask=0.0,\n",
    "    # Set both of these to True to include photometry in fitting\n",
    "    include_photometry=False,  \n",
    "    use_phot_obj=False,\n",
    ")\n",
    "\n",
    "obj_id = 2663\n",
    "obj_z = 2.6724\n",
    "\n",
    "beams = grp.get_beams(\n",
    "    obj_id,\n",
    "    size=50, \n",
    "    min_mask=0,\n",
    "    min_sens=0,\n",
    "    show_exception=True,\n",
    "    beam_id=\"A\",\n",
    ")\n",
    "mb = multifit.MultiBeam(\n",
    "    beams, fcontam=0.2, min_sens=0.0, min_mask=0, group_name=root_name\n",
    ")\n",
    "mb.fit_trace_shift()\n",
    "mb.write_master_fits()\n",
    "\n",
    "_ = fitting.run_all_parallel(\n",
    "    obj_id,\n",
    "    zr=[obj_z, obj_z + 0.01],\n",
    "    verbose=True,\n",
    "    get_output_data=True,\n",
    "    skip_complete=False,\n",
    "    save_figures=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12_GLASS_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
