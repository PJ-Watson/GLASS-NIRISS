{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "root_dir = Path(\"/media\") / \"sharedData\" / \"data\"\n",
    "root_name = \"glass-a2744\"\n",
    "\n",
    "catalogue_dir = root_dir / \"2024_08_16_A2744_v4\" / \"glass_niriss\" / \"match_catalogues\"\n",
    "\n",
    "grizli_home_dir = root_dir / \"2024_08_16_A2744_v4\" / \"grizli_home\"\n",
    "grizli_extraction_dir = grizli_home_dir / \"Extractions_v2\"\n",
    "os.chdir(grizli_extraction_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grizli import jwst_utils, multifit\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "jwst_utils.QUIET_LEVEL = logging.WARNING\n",
    "jwst_utils.set_quiet_logging(jwst_utils.QUIET_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, join\n",
    "import numpy as np\n",
    "\n",
    "cat_v1 = Table.read(catalogue_dir / \"classification_v1\" / \"pyGCG_output_BV.fits\")\n",
    "phot_cat = Table.read(catalogue_dir / \"grizli_photz_matched.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (np.nansum(np.isin(cat_v1[\"ID\"],phot_cat[\"NUMBER\"])))\n",
    "\n",
    "cat_v1.remove_columns([\"RA\",\"DEC\"])\n",
    "\n",
    "cat_v1[\"NUMBER\"] = cat_v1[\"ID\"].astype(int)\n",
    "cat_v2 = join(phot_cat, cat_v1, keys=\"NUMBER\", join_type=\"left\")\n",
    "cat_v2[\"V1_CLASS\"] = 10\n",
    "\n",
    "# cat_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "\n",
    "filters = [\"F115W\",\"F150W\",\"F200W\"]\n",
    "pas = [\"72.0\",\"341.0\"]\n",
    "\n",
    "beam_names = [f\"{a},{b}\" for b, a in product(pas, filters)]\n",
    "coverage_names = [f\"{b}_COVERAGE\" for b in beam_names]\n",
    "quality_names = [f\"{b}_QUALITY\" for b in beam_names]\n",
    "new_names = [f\"{b}_USE\" for b in beam_names]\n",
    "\n",
    "missing_ids = ~np.isin(cat_v2[\"NUMBER\"],cat_v1[\"ID\"])\n",
    "\n",
    "cat_v2[\"V1_CLASS\"][missing_ids] = 0\n",
    "\n",
    "for q in new_names:\n",
    "    cat_v2[q] = 1\n",
    "    cat_v2[q][missing_ids] = 0\n",
    "\n",
    "# cat_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "misplaced_ids = np.array([1,8,10,17,52,63,77,99,104,115,120,174])\n",
    "\n",
    "for i_p, pa in enumerate(pas):\n",
    "    unusable_f150w_flag = (\n",
    "        (cat_v2[f\"F150W,{pa}_QUALITY\"]==\"Unusable\")\n",
    "        & (cat_v2[f\"F150W,{pa}_COVERAGE\"]!=0)\n",
    "        & ((cat_v2[\"NUMBER\"].astype(int)<260) & (~np.isin(cat_v2[\"NUMBER\"].astype(int), misplaced_ids))) \n",
    "\n",
    "    )\n",
    "    for filt in filters:\n",
    "        cat_v2[f\"{filt},{pa}_USE\"][unusable_f150w_flag] = 0\n",
    "\n",
    "    unusable_f115w_flag = (\n",
    "        (cat_v2[f\"F115W,{pa}_QUALITY\"]==\"Unusable\")\n",
    "        & (cat_v2[f\"F115W,{pa}_COVERAGE\"]!=0)\n",
    "        & ((cat_v2[\"NUMBER\"].astype(int)<260) & (~np.isin(cat_v2[\"NUMBER\"].astype(int), misplaced_ids)))\n",
    "    )\n",
    "    for filt in filters:\n",
    "        cat_v2[f\"{filt},{pa}_USE\"][unusable_f115w_flag] = 0\n",
    "        # cat_v2[f\"{filt},{pa}_USE\"][np.isin(cat_v2[\"NUMBER\"],cat_v1[\"ID\"][unusable_f115w_flag])] = 0\n",
    "\n",
    "# cat_v2[\"ESTIMATED_REDSHIFT\"] = np.nan\n",
    "# cat_v2[\"ESTIMATED_REDSHIFT\"][np.isin(cat_v2[\"NUMBER\"],cat_v1[\"ID\"])] = cat_v1[\"ESTIMATED_REDSHIFT\"]\n",
    "\n",
    "# print (np.where(cat_v2[\"NUMBER\"]==cat_v1[\"ID\"]))\n",
    "# a,b,v1_to_v2_idx = np.intersect1d(cat_v2[\"NUMBER\"],cat_v1[\"ID\"], return_indices=True)\n",
    "# cat_v2[\"ESTIMATED_REDSHIFT\"][b] = cat_v1[\"ESTIMATED_REDSHIFT\"]\n",
    "# cat_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_p, pa in enumerate(pas):\n",
    "    for filt in filters:\n",
    "        unusable_flag = (\n",
    "            ((cat_v2[\"NUMBER\"].astype(int)>=260) | (np.isin(cat_v2[\"NUMBER\"].astype(int), misplaced_ids))) &\n",
    "            (\n",
    "                (cat_v2[f\"{filt},{pa}_QUALITY\"]==\"Unusable\")\n",
    "                | (cat_v2[f\"{filt},{pa}_COVERAGE\"]==0)\n",
    "            )\n",
    "        )\n",
    "        cat_v2[f\"{filt},{pa}_USE\"][unusable_flag] = 0\n",
    "        \n",
    "# cat_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1_CLASS:\n",
    "# 0 - Extraction not possible\n",
    "# 1 - Unusable in all beams\n",
    "# 2 - Star\n",
    "# 3 - Bad seg map\n",
    "# 4 - Some bad beams, re-extract\n",
    "# 5 - No bad beams, re-extract\n",
    "# 6 - No bad beams, specz ~= estimated z (don't re-extract)\n",
    "\n",
    "for row in cat_v2[~cat_v2[\"COMMENTS\"].mask]:\n",
    "    if \"star\" in row[\"COMMENTS\"]:\n",
    "        cat_v2[\"V1_CLASS\"][np.isin(cat_v2[\"NUMBER\"],int(row[\"ID\"]))] = 2\n",
    "    elif \"unusable\" in row[\"COMMENTS\"]:\n",
    "        # print (row)\n",
    "        if \"72\" in row[\"COMMENTS\"]:\n",
    "            for filt in filters:\n",
    "                cat_v2[f\"{filt},72.0_USE\"][np.isin(cat_v2[\"NUMBER\"],int(row[\"ID\"]))] = 0\n",
    "        elif \"341\" in row[\"COMMENTS\"]:\n",
    "            for filt in filters:\n",
    "                cat_v2[f\"{filt},341.0_USE\"][np.isin(cat_v2[\"NUMBER\"],int(row[\"ID\"]))] = 0\n",
    "    elif \"remove\" in row[\"COMMENTS\"]:\n",
    "        for pa in pas:\n",
    "            for filt in filters:\n",
    "                cat_v2[f\"{filt},{pa}_USE\"][np.isin(cat_v2[\"NUMBER\"],int(row[\"ID\"]))] = 0\n",
    "    else:\n",
    "        pass\n",
    "        # print (row[\"ID\"],row[\"COMMENTS\"])\n",
    "\n",
    "cat_v2[\"V1_CLASS\"][\n",
    "    np.all(structured_to_unstructured(np.array(cat_v2[new_names]))==0, axis=1)\n",
    "    & (cat_v2[\"V1_CLASS\"]==10)\n",
    "] = 1\n",
    "\n",
    "cat_v2[\"REDSHIFT_USE\"] = cat_v2[\"ESTIMATED_REDSHIFT\"]\n",
    "\n",
    "# cat_v2\n",
    "\n",
    "# print (np.unique(cat_v2[\"V1_CLASS\"], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_v2[\"V1_CLASS\"][(cat_v2[\"BAD_SEG_MAP\"] == True) & (cat_v2[\"V1_CLASS\"] == 10)] = 3\n",
    "\n",
    "\n",
    "cat_v2[\"V1_CLASS\"][\n",
    "    np.any(structured_to_unstructured(np.array(cat_v2[new_names])) == 0, axis=1)\n",
    "    & (cat_v2[\"V1_CLASS\"] == 10)\n",
    "] = 4\n",
    "\n",
    "cat_v2[\"V1_CLASS\"][\n",
    "    np.all(structured_to_unstructured(np.array(cat_v2[new_names])) == 1, axis=1)\n",
    "    & (cat_v2[\"V1_CLASS\"] == 10)\n",
    "] = 5\n",
    "\n",
    "\n",
    "cat_v2[\"V1_CLASS\"][\n",
    "    (cat_v2[\"V1_CLASS\"] == 5)\n",
    "    # (cat_v2[\"ESTIMATED_REDSHIFT\"] == cat_v2[\"GRIZLI_REDSHIFT\"])\n",
    "    & (cat_v2[\"zspec\"] > 0)\n",
    "    & (np.abs(cat_v2[\"ESTIMATED_REDSHIFT\"]/cat_v2[\"GRIZLI_REDSHIFT\"]-1)<=0.005)\n",
    "    # & (~np.any(structured_to_unstructured(np.array(cat_v2[new_names])) == 0))\n",
    "] = 6\n",
    "\n",
    "# print(np.nansum(cat_v2[\"zspec\"] > 0))\n",
    "\n",
    "# print (np.abs(cat_v2[\"ESTIMATED_REDSHIFT\"]/cat_v2[\"GRIZLI_REDSHIFT\"])[cat_v2[\"V1_CLASS\"]==6])\n",
    "# print (quality_names)\n",
    "# print (np.nansum(\n",
    "#     (cat_v1[quality_names[1]]==\"Unusable\") &\n",
    "#     (cat_v1[coverage_names[1]]!=0)\n",
    "# ))\n",
    "\n",
    "# print (np.unique(cat_v2[\"V1_CLASS\"], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Statistics:\")\n",
    "# print(f\"Total: {len(cat_v2)}\")\n",
    "# print(\n",
    "#     f\"Usable in at least one beam: {np.nansum(~np.all(structured_to_unstructured(np.array(cat_v2[new_names]))==0, axis=1))}\"\n",
    "# )\n",
    "# print(\n",
    "#     f\"Usable in all beams: {np.nansum(np.all(structured_to_unstructured(np.array(cat_v2[new_names]))==1, axis=1))}\"\n",
    "# )\n",
    "# print(\"\")\n",
    "# print(f\"No extraction possible: {np.nansum(cat_v2[\"V1_CLASS\"]==0)}\")\n",
    "# print(f\"Unusable in all orientations: {np.nansum(cat_v2[\"V1_CLASS\"]==1)}\")\n",
    "# print(f\"Star: {np.nansum(cat_v2[\"V1_CLASS\"]==2)}\")\n",
    "# print(f\"Bad seg map: {np.nansum(cat_v2[\"V1_CLASS\"]==3)}\")\n",
    "# print(f\"Unusable in some orientations: {np.nansum(cat_v2[\"V1_CLASS\"]==4)}\")\n",
    "# print(f\"Good in all orientations: {np.nansum(cat_v2[\"V1_CLASS\"]==5)}\")\n",
    "# print(f\"Good in all orientations, and specz: {np.nansum(cat_v2[\"V1_CLASS\"]==6)}\")\n",
    "# print(\"\")\n",
    "\n",
    "# use_cat = cat_v2[cat_v2[\"V1_CLASS\"] >= 4]\n",
    "\n",
    "# print (np.nansum(use_cat[\"UNRELIABLE_REDSHIFT\"]))\n",
    "# print (np.nansum(use_cat[\"zphot\"]>=3))\n",
    "# print (np.nansum(use_cat[\"zphot\"]<2))\n",
    "\n",
    "\n",
    "# print (\"change grizli to highz:\", np.nansum((use_cat[\"GRIZLI_REDSHIFT\"]!=use_cat[\"ESTIMATED_REDSHIFT\"]) & (use_cat[\"ESTIMATED_REDSHIFT\"]>=6)))\n",
    "# print (\"change photz to highz:\", np.nansum((use_cat[\"zphot\"]<6) & (use_cat[\"ESTIMATED_REDSHIFT\"]>=6)))\n",
    "# print (np.nansum((use_cat[\"UNRELIABLE_REDSHIFT\"] == True) & (np.isfinite(use_cat[\"zphot\"]))))\n",
    "# use_cat[(use_cat[\"UNRELIABLE_REDSHIFT\"] == True) & (np.isfinite(use_cat[\"zphot\"])) & (use_cat[\"zphot\"]>=5)]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(\n",
    "#     use_cat[\"zphot\"][(use_cat[\"UNRELIABLE_REDSHIFT\"] == True) & (np.isfinite(use_cat[\"zphot\"]))]\n",
    "# )\n",
    "\n",
    "\n",
    "# highz_cat = use_cat[\n",
    "#     ((use_cat[\"REDSHIFT_USE\"] >= 6) | (use_cat[\"UNRELIABLE_REDSHIFT\"] == True) )\n",
    "#     & (np.isfinite(use_cat[\"zphot\"]))\n",
    "# ]\n",
    "\n",
    "# lowz_cat = use_cat[\n",
    "#     ((use_cat[\"REDSHIFT_USE\"] < 6) & (use_cat[\"UNRELIABLE_REDSHIFT\"] == False))\n",
    "#     | (~np.isfinite(use_cat[\"zphot\"]))\n",
    "# ]\n",
    "\n",
    "# print(f\"Number of z>=6: {len(highz_cat)}\")\n",
    "# print(f\"Number of z<6: {len(lowz_cat)}\")\n",
    "# print(\"\")\n",
    "# print(\n",
    "#     f\"Number of z>=6 and not unreliable: \",\n",
    "#     np.nansum((highz_cat[\"UNRELIABLE_REDSHIFT\"] == False) | (highz_cat[\"TENTATIVE_REDSHIFT\"]==True))\n",
    "# )\n",
    "# print (highz_cat[highz_cat[\"UNRELIABLE_REDSHIFT\"] == False])\n",
    "# print(\n",
    "#     f\"Number of z<6 and tentative: \",\n",
    "#     np.nansum(lowz_cat[\"TENTATIVE_REDSHIFT\"] == True)\n",
    "# )\n",
    "# print (lowz_cat[lowz_cat[\"TENTATIVE_REDSHIFT\"] == True])\n",
    "\n",
    "# cat_out_path = catalogue_dir / \"classification_v1\" / \"compiled_catalogue_v1.fits\"\n",
    "# if not cat_out_path.is_file():\n",
    "#     cat_v2.write(catalogue_dir / \"classification_v1\" / \"compiled_catalogue_v1.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matt 220\n",
      "Tommaso 184\n",
      "Peter 582\n",
      "Nicolo 450\n",
      "Xianlong 450\n",
      "Yechi 550\n",
      "Guido 560\n",
      "Sofia 559\n",
      "\n",
      "Low redshift:\n",
      "Total:\t1024\n",
      "Viewed 1 time(s): 525\n",
      "Viewed 2 time(s): 499\n",
      "\n",
      "High redshift:\n",
      "Total:\t1016\n",
      "Viewed 2 time(s): 1016\n"
     ]
    }
   ],
   "source": [
    "highz_names = {\n",
    "    \"Guido\": 0.8,\n",
    "    \"Sofia\": 0.8,\n",
    "    \"Yeichi\": 0.2,\n",
    "    \"Tommaso\": 0.2,\n",
    "}\n",
    "\n",
    "lowz_names = {\n",
    "    \"Peter\": 0.8,\n",
    "    \"Matt\": 0.2,\n",
    "    \"Xianlong\": 0.5,\n",
    "    \"Nicola\": 0.5,\n",
    "}\n",
    "highz_bool = (cat_v2[\"V1_CLASS\"] >= 4) & (\n",
    "    ((cat_v2[\"REDSHIFT_USE\"] >= 6) | (cat_v2[\"UNRELIABLE_REDSHIFT\"] == True))\n",
    "    & (~cat_v2[\"zphot\"].mask)\n",
    ")\n",
    "highz_idxs = np.where(highz_bool)[0]\n",
    "\n",
    "lowz_bool = (cat_v2[\"V1_CLASS\"] >= 4) & (\n",
    "    ((cat_v2[\"REDSHIFT_USE\"] < 6) & (cat_v2[\"UNRELIABLE_REDSHIFT\"] == False))\n",
    "    | (cat_v2[\"zphot\"].mask)\n",
    ")\n",
    "lowz_idxs = np.where(lowz_bool)[0]\n",
    "\n",
    "cat_v2[\"zphot\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "cat_idxs = {}\n",
    "\n",
    "cat_idxs[\"Matt\"] = (lowz_bool & cat_v2[\"TENTATIVE_REDSHIFT\"]) | np.array(\n",
    "    [\n",
    "        True if i in lowz_idxs[rng.random(len(lowz_idxs)) < 0.1] else False\n",
    "        for i in np.arange(len(cat_v2))\n",
    "    ]\n",
    ")\n",
    "cat_idxs[\"Tommaso\"] = (highz_bool) & (\n",
    "    cat_v2[\"TENTATIVE_REDSHIFT\"]\n",
    "    | (cat_v2[\"UNRELIABLE_REDSHIFT\"] == False)\n",
    "    | np.array(\n",
    "        [\n",
    "            True if i in highz_idxs[rng.random(len(highz_idxs)) < 0.15] else False\n",
    "            for i in np.arange(len(cat_v2))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "cat_idxs[\"Peter\"] = (\n",
    "    (lowz_bool) & (cat_v2[\"TENTATIVE_REDSHIFT\"] | (cat_v2[\"zphot\"].mask))\n",
    "    | (\n",
    "        np.array(\n",
    "            [\n",
    "                True if i in highz_idxs[rng.random(len(highz_idxs)) < 0.2] else False\n",
    "                for i in np.arange(len(cat_v2))\n",
    "            ]\n",
    "        )\n",
    "        & (~cat_idxs[\"Tommaso\"])\n",
    "    )\n",
    ")\n",
    "nicolo_no_z_idxs = (lowz_bool) & (cat_v2[\"zphot\"].mask) & (~cat_idxs[\"Matt\"])\n",
    "assigned_nicolo_no_z = np.arange(len(cat_v2))[nicolo_no_z_idxs]\n",
    "unassigned_nicolo = np.arange(len(cat_v2))[\n",
    "    (~cat_idxs[\"Matt\"]) & (~cat_idxs[\"Peter\"]) & lowz_bool\n",
    "]\n",
    "cat_idxs[\"Nicolo\"] = np.isin(\n",
    "    np.arange(len(cat_v2)),\n",
    "    np.concatenate([assigned_nicolo_no_z, rng.choice(unassigned_nicolo, 450-len(assigned_nicolo_no_z), replace=False)])\n",
    ")\n",
    "assigned_xianlong = np.arange(len(cat_v2))[\n",
    "    (((~cat_idxs[\"Matt\"] & cat_idxs[\"Peter\"])) | (cat_idxs[\"Nicolo\"])) & lowz_bool & (~nicolo_no_z_idxs)\n",
    "]\n",
    "unassigned_xianlong = np.arange(len(cat_v2))[\n",
    "    (~cat_idxs[\"Matt\"]) & (~cat_idxs[\"Peter\"]) & (~cat_idxs[\"Nicolo\"]) & lowz_bool\n",
    "]\n",
    "cat_idxs[\"Xianlong\"] = np.isin(\n",
    "    np.arange(len(cat_v2)),\n",
    "    np.concatenate([unassigned_xianlong, rng.choice(assigned_xianlong, 450-len(unassigned_xianlong), replace=False)])\n",
    ")\n",
    "unassigned_yechi = np.arange(len(cat_v2))[\n",
    "    (~cat_idxs[\"Tommaso\"]) & (~cat_idxs[\"Peter\"]) & highz_bool\n",
    "]\n",
    "\n",
    "cat_idxs[\"Yechi\"] =   np.isin(\n",
    "        np.arange(len(cat_v2)),\n",
    "        rng.choice(unassigned_yechi, 550, replace=False)\n",
    "    )\n",
    "assigned_g = (\n",
    "    (~cat_idxs[\"Tommaso\"]) & (~cat_idxs[\"Peter\"]) & (~cat_idxs[\"Yechi\"]) & highz_bool\n",
    ")\n",
    "not_assigned_idxs = np.where((~assigned_g) & highz_bool)[0]\n",
    "temp_g = np.isin(np.arange(len(cat_v2)), rng.choice(not_assigned_idxs, 560-np.nansum(assigned_g), replace=False))\n",
    "# temp_g = np.array(\n",
    "#     [\n",
    "#         (\n",
    "#             True\n",
    "#             if i in not_assigned_idxs[rng.random(len(not_assigned_idxs)) < 0.5]\n",
    "#             else False\n",
    "#         )\n",
    "#         for i in np.arange(len(cat_v2))\n",
    "#     ]\n",
    "# )\n",
    "cat_idxs[\"Guido\"] = assigned_g | temp_g\n",
    "cat_idxs[\"Sofia\"] = (highz_bool) & ~temp_g\n",
    "\n",
    "for k, v in cat_idxs.items():\n",
    "    print(k, np.nansum(v))\n",
    "\n",
    "lowz_array = np.array(\n",
    "    [cat_idxs[\"Matt\"], cat_idxs[\"Peter\"], cat_idxs[\"Nicolo\"], cat_idxs[\"Xianlong\"]]\n",
    ")\n",
    "print(f\"\\nLow redshift:\\nTotal:\\t{len(lowz_idxs)}\")\n",
    "for v, c in zip(\n",
    "    *np.unique(np.nansum(lowz_array[:, lowz_bool], axis=0), return_counts=True)\n",
    "):\n",
    "    print(f\"Viewed {v} time(s): {c:0<3}\")\n",
    "\n",
    "highz_array = np.array(\n",
    "    [\n",
    "        cat_idxs[\"Tommaso\"],\n",
    "        cat_idxs[\"Yechi\"],\n",
    "        cat_idxs[\"Guido\"],\n",
    "        cat_idxs[\"Sofia\"],\n",
    "        cat_idxs[\"Peter\"],\n",
    "    ]\n",
    ")\n",
    "print(f\"\\nHigh redshift:\\nTotal:\\t{len(highz_idxs)}\")\n",
    "for v, c in zip(\n",
    "    *np.unique(np.nansum(highz_array[:, highz_bool], axis=0), return_counts=True)\n",
    "):\n",
    "    print(f\"Viewed {v} time(s): {c:0<3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matt 220\n",
      "Tommaso 184\n",
      "Peter 582\n",
      "Nicolo 450\n",
      "Xianlong 450\n",
      "Yechi 550\n",
      "Guido 560\n",
      "Sofia 559\n"
     ]
    }
   ],
   "source": [
    "new_cat_dir = grizli_home_dir / \"classification-stage-2\" / \"catalogues\" / \"input\"\n",
    "new_cat_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for p, idxs in cat_idxs.items():\n",
    "    if not (new_cat_dir / f\"input_cat_{p}.fits\").is_file():\n",
    "        p_cat = cat_v2[idxs].copy()\n",
    "        p_cat.write(new_cat_dir / f\"input_cat_{p}.fits\", overwrite=True)\n",
    "        print (p, len(p_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grizli import fitting\n",
    "from grizli.pipeline import auto_script\n",
    "\n",
    "reextract_dir = Path.cwd() / \"reextracted\"\n",
    "reextract_dir.mkdir(exist_ok=True, parents=True)\n",
    "skipped_dir = Path.cwd() / \"skipped\"\n",
    "skipped_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for filetype in [\"beams\", \"full\", \"1D\", \"row\", \"line\", \"log_par\", \"stack\"]:\n",
    "    (reextract_dir / filetype).mkdir(exist_ok=True, parents=True)\n",
    "    (skipped_dir / filetype).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "specz_dir = grizli_home_dir / \"Extractions\" / \"specz\"\n",
    "photz_dir = grizli_home_dir / \"Extractions\" / \"photz\"\n",
    "\n",
    "max_size = 450\n",
    "\n",
    "for row in cat_v2[:]:\n",
    "    obj_id = row[\"NUMBER\"]\n",
    "    obj_z = row[\"REDSHIFT_USE\"]\n",
    "\n",
    "    if row[\"V1_CLASS\"] == 6:\n",
    "        if not (skipped_dir / \"full\" / f\"{root_name}_{obj_id:05}.full.fits\").is_file():\n",
    "            for filetype in [\"beams\", \"full\", \"1D\", \"row\", \"line\", \"log_par\", \"stack\"]:\n",
    "                try:\n",
    "                    [\n",
    "                        shutil.copy2(p, (skipped_dir / filetype / p.name))\n",
    "                        for p in (specz_dir / filetype).glob(\n",
    "                            f\"*{obj_id:05}.*{filetype}*\"\n",
    "                        )\n",
    "                    ]\n",
    "                except:\n",
    "                    [\n",
    "                        shutil.copy2(p, (skipped_dir / filetype / p.name))\n",
    "                        for p in photz_dir.glob(f\"*{obj_id:05}.*{filetype}*\")\n",
    "                    ]\n",
    "\n",
    "    if not ((row[\"V1_CLASS\"] == 4) | (row[\"V1_CLASS\"] == 5)):\n",
    "        continue\n",
    "\n",
    "    if (reextract_dir / \"full\" / f\"{root_name}_{obj_id:05}.full.fits\").is_file():\n",
    "        continue\n",
    "    # # print (obj_id, obj_z, row[\"FLUX_AUTO\"])\n",
    "    # # continue\n",
    "    # Maximum diagonal extent of detection bounding box, measured from centre\n",
    "    # det_diag = np.sqrt((row[\"XMAX\"] - row[\"XMIN\"])**2 + (row[\"YMAX\"] - row[\"YMIN\"])**2)\n",
    "    det_halfdiag = np.sqrt(\n",
    "        (np.nanmax([row[\"XMAX\"] - row[\"X\"], row[\"X\"] - row[\"XMIN\"]])) ** 2\n",
    "        + (np.nanmax([row[\"YMAX\"] - row[\"Y\"], row[\"Y\"] - row[\"YMIN\"]])) ** 2\n",
    "    )\n",
    "\n",
    "    # pixel scale is half detection\n",
    "    # Include factor of 25% to account for blotting and pixelation effects\n",
    "    est_beam_size = np.clip(\n",
    "        int(np.nanmin([np.ceil(0.5 * 1.25 * det_halfdiag), max_size])),\n",
    "        a_min=5,\n",
    "        a_max=max_size,\n",
    "    )\n",
    "\n",
    "    pline = {\n",
    "        \"kernel\": \"square\",\n",
    "        \"pixfrac\": 1.0,\n",
    "        \"pixscale\": 0.06,\n",
    "        \"size\": int(np.clip(2 * est_beam_size * 0.06, a_min=3, a_max=30)),\n",
    "        \"wcs\": None,\n",
    "    }\n",
    "    args = auto_script.generate_fit_params(\n",
    "        pline=pline,\n",
    "        field_root=root_name,\n",
    "        min_sens=0.0,\n",
    "        min_mask=0.0,\n",
    "        include_photometry=False,  # set both of these to True to include photometry in fitting\n",
    "        use_phot_obj=False,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            mb = multifit.MultiBeam(\n",
    "                str(specz_dir / \"beams\" / f\"{root_name}_{obj_id:05}.beams.fits\"),\n",
    "                fcontam=0.2,\n",
    "                min_sens=0.0,\n",
    "                min_mask=0,\n",
    "                group_name=root_name,\n",
    "            )\n",
    "        except:\n",
    "            mb = multifit.MultiBeam(\n",
    "                str(photz_dir / \"beams\" / f\"{root_name}_{obj_id:05}.beams.fits\"),\n",
    "                fcontam=0.2,\n",
    "                min_sens=0.0,\n",
    "                min_mask=0,\n",
    "                group_name=root_name,\n",
    "            )\n",
    "        # dir(mb)\n",
    "        # print (mb.beams)\n",
    "        # print (mb.PA)\n",
    "        new_beam_list = []\n",
    "        for filt, pa_dict in mb.PA.items():\n",
    "            # print (filt, pa_dict)\n",
    "            for pa, beam_idxs in pa_dict.items():\n",
    "                if row[f\"{filt},{pa}_USE\"] == 1:\n",
    "                    [new_beam_list.append(mb.beams[b]) for b in beam_idxs]\n",
    "\n",
    "        # print (dir(new_beam_list[0]))\n",
    "        # print (new_beam_list[0].direct.filter)\n",
    "        # exit()\n",
    "        new_mb = multifit.MultiBeam(\n",
    "            new_beam_list, fcontam=0.2, min_sens=0.0, min_mask=0, group_name=root_name\n",
    "        )\n",
    "        # print (new_mb.beams[0].direct.filter)\n",
    "        # for beam in new_beam_list:\n",
    "        #     beam.direct.filter = beam.direct.filter.split(\"-\")[-1]\n",
    "        #     beam.grism.filter = beam.grism.filter.split(\"-\")[-1]\n",
    "        new_mb.write_master_fits()\n",
    "\n",
    "        del mb, new_mb\n",
    "\n",
    "        if np.isfinite(obj_z):\n",
    "            zr = [obj_z * 0.8, obj_z * 1.2]\n",
    "        else:\n",
    "            zr = [0, 10.0]\n",
    "\n",
    "        #     # mb.write_master_fits()\n",
    "        # #     shutil.copy2(specz_dir / \"beams\" / f\"{root_name}_{obj_id:05}.beams.fits\", f\"{root_name}_{obj_id:05}.beams.fits\")\n",
    "        # # except:\n",
    "        # #     shutil.copy2(photz_dir / \"beams\" / f\"{root_name}_{obj_id:05}.beams.fits\", f\"{root_name}_{obj_id:05}.beams.fits\")\n",
    "        print(f\"Fitting {obj_id}...\")\n",
    "\n",
    "        _ = fitting.run_all_parallel(\n",
    "            obj_id,\n",
    "            zr=zr,\n",
    "            dz=[0.01, 0.001],\n",
    "            verbose=True,\n",
    "            get_output_data=True,\n",
    "            skip_complete=False,\n",
    "            save_figures=True,\n",
    "        )\n",
    "        print(\"Fit complete, output saved.\")\n",
    "        for filetype in [\"beams\", \"full\", \"1D\", \"row\", \"line\", \"log_par\", \"stack\"]:\n",
    "            [\n",
    "                p.rename(reextract_dir / filetype / p.name)\n",
    "                for p in Path.cwd().glob(f\"*{obj_id}.*{filetype}*\")\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction failed for {obj_id}.\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from astropy.io import fits\n",
    "# from astropy.table import Table, vstack, join\n",
    "# from astropy.coordinates import match_coordinates_sky, SkyCoord\n",
    "# from astropy.wcs import WCS\n",
    "# import astropy.visualization as astrovis\n",
    "# import astropy.units as u\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# plot = True\n",
    "\n",
    "# if not (catalogue_dir / \"ASTRODEEP_photz.fits\").is_file():\n",
    "\n",
    "#     uncover_cat = Table.read(catalogue_dir / \"UNCOVER_DR3_SPS_redshift_catalog.fits\")\n",
    "\n",
    "#     uncover_cat.rename_columns([\"id\", \"z_ml\"], [\"id_uncover\", \"zphot\"])\n",
    "\n",
    "#     astrodeep_cat = Table.read(catalogue_dir / \"ASTRODEEP-JWST-ABELL2744_photoz.fits\")\n",
    "\n",
    "#     astrodeep_cat.rename_columns([\"ID\", \"RA\", \"DEC\"], [\"id_astrodeep_zphot\", \"ra\", \"dec\"])\n",
    "#     # alt_cat.rename_columns([\"id\", \"z_ALT\"], [\"id_alt\", \"zspec\"])\n",
    "#     # max_offset = 0.1 * u.arcsec\n",
    "\n",
    "#     # use_idx = sep2d > max_offset\n",
    "\n",
    "#     # astrodeep_cat = astrodeep_cat\n",
    "\n",
    "#     # combined_spec_cat = vstack(\n",
    "#     #     [astrodeep_cat[astrodeep_cat[\"zspec\"] > -99], alt_cat[use_idx]]\n",
    "#     # )\n",
    "#     grizli_zspec_cat = Table.read(catalogue_dir / \"grizli_specz_matched.fits\")\n",
    "\n",
    "#     # print (np.nansum(np.isin(astrodeepgrizli_zspec_cat[\"id_astrodeep\"])))\n",
    "\n",
    "#     # grp_idx, grp_sep2d = grp.catalog.match_to_catalog_sky(uncover_cat)\n",
    "#     grp_idx, grp_sep2d = grp.catalog.match_to_catalog_sky(astrodeep_cat)\n",
    "\n",
    "#     astrodeep_cat[\"NUMBER\"] = np.full_like(\n",
    "#         astrodeep_cat[\"ra\"].shape, -99, dtype=int\n",
    "#     )\n",
    "#     uncover_cat[\"NUMBER\"] = np.full_like(\n",
    "#         uncover_cat[\"ra\"].shape, -99, dtype=int\n",
    "#     )\n",
    "#     # print (grp.catalog.colnames)\n",
    "\n",
    "#     print(len(np.unique(grp_idx)))\n",
    "#     print(grp_idx)\n",
    "#     unique_matches = np.unique(grp_idx)\n",
    "#     for un in unique_matches:\n",
    "#         match_idxs = np.argwhere((grp_idx == un) & (grp_sep2d < 1 * u.arcsec))\n",
    "#         if len(match_idxs) == 0:\n",
    "#             continue\n",
    "#         # print(grp_idx[match_idxs])\n",
    "#         if len(match_idxs) >= 2:\n",
    "#             match_idxs = match_idxs[np.argmin(grp_sep2d[match_idxs])]\n",
    "#         astrodeep_cat[\"NUMBER\"][match_idxs] = grp.catalog[\"NUMBER\"][un]\n",
    "#         # uncover_cat[\"NUMBER\"][match_idxs] = grp.catalog[\"NUMBER\"][un]\n",
    "\n",
    "#     # grizli_cat = Table.read(f\"{root_name}-ir.cat.fits\")\n",
    "#     del grizli_zspec_cat[\"zphot\"]\n",
    "#     grizli_zphot_cat = join(grizli_zspec_cat, astrodeep_cat[\"NUMBER\", \"id_astrodeep_zphot\", \"zphot\"], keys=\"NUMBER\", join_type=\"left\")\n",
    "#     plot=False\n",
    "#     if plot:\n",
    "#         with fits.open(\n",
    "#             grizli_extraction_dir.parent / \"Prep\" / f\"{root_name}-ir_drc_sci.fits\"\n",
    "#         ) as hdul:\n",
    "#             direct_img = hdul[0].data.copy()\n",
    "#             direct_wcs = WCS(hdul[0].header.copy())\n",
    "#             del hdul\n",
    "\n",
    "#         # mismatched_spec_cat = combined_spec_cat[\n",
    "#         #     # (grp_sep2d>1*u.arcsec)\n",
    "#         #     (grp_sep2d <= 1 * u.arcsec)\n",
    "#         #     # & (grp_sep2d > 0.75 * u.arcsec)\n",
    "#         # ]\n",
    "#         # print(len(mismatched_spec_cat))\n",
    "\n",
    "#         fig, ax = plt.subplots(\n",
    "#             # figsize=(20, 15),\n",
    "#             dpi=600,\n",
    "#             subplot_kw={\"projection\": direct_wcs},\n",
    "#         )\n",
    "#         ax.imshow(\n",
    "#             direct_img,\n",
    "#             norm=astrovis.ImageNormalize(\n",
    "#                 direct_img,\n",
    "#                 stretch=astrovis.LogStretch(),\n",
    "#                 interval=astrovis.PercentileInterval(99.9),\n",
    "#             ),\n",
    "#             cmap=\"binary\",\n",
    "#         )\n",
    "#         xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "#         ax.scatter(\n",
    "#             grizli_zphot_cat[\"ra\"],\n",
    "#             grizli_zphot_cat[\"dec\"],\n",
    "#             transform=ax.get_transform(\"world\"),\n",
    "#             c=grizli_zphot_cat[\"zphot\"],\n",
    "#             s=1,\n",
    "#             vmin=0.25,vmax=0.35, cmap=\"rainbow\"\n",
    "#         )\n",
    "#         ax.set_xlim(xlim), ax.set_ylim(ylim)\n",
    "# # else:\n",
    "#     # combined_photz_cat = Table.read(catalogue_dir / \"ASTRODEEP_phostz.fits\")\n",
    "#     grizli_zphot_cat.write(catalogue_dir / \"grizli_photz_matched.fits\", overwrite=True)\n",
    "# else:\n",
    "#     grizli_zphot_cat = Table.read(catalogue_dir / \"grizli_photz_matched.fits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grizli_zphot_cat.sort(\"FLUX_AUTO\", reverse=True)\n",
    "\n",
    "# photz_dir = Path.cwd() / \"photz\"\n",
    "# photz_dir.mkdir(exist_ok=True, parents=True)\n",
    "# for filetype in [\"beams\", \"full\", \"1D\", \"row\", \"line\", \"log_par\", \"stack\"]:\n",
    "#     (photz_dir / filetype).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# max_size = 450\n",
    "\n",
    "# for row in grizli_zphot_cat[:]:\n",
    "#     obj_id = row[\"NUMBER\"]\n",
    "#     obj_z = row[\"zphot\"]\n",
    "\n",
    "#     if (\n",
    "#         grizli_extraction_dir / \"specz\" / \"full\" / f\"{root_name}_{obj_id:05}.full.fits\"\n",
    "#     ).is_file() or (\n",
    "#         photz_dir / \"full\" / f\"{root_name}_{obj_id:05}.full.fits\"\n",
    "#     ).is_file():\n",
    "#         continue\n",
    "#     if obj_id==2409:\n",
    "#         continue\n",
    "#     # Maximum diagonal extent of detection bounding box, measured from centre\n",
    "#     # det_diag = np.sqrt((row[\"XMAX\"] - row[\"XMIN\"])**2 + (row[\"YMAX\"] - row[\"YMIN\"])**2)\n",
    "#     det_halfdiag = np.sqrt(\n",
    "#         (np.nanmax([row[\"XMAX\"] - row[\"X\"], row[\"X\"] - row[\"XMIN\"]])) ** 2\n",
    "#         + (np.nanmax([row[\"YMAX\"] - row[\"Y\"], row[\"Y\"] - row[\"YMIN\"]])) ** 2\n",
    "#     )\n",
    "\n",
    "#     # pixel scale is half detection\n",
    "#     # Include factor of 25% to account for blotting and pixelation effects\n",
    "#     est_beam_size = int(np.nanmin([np.ceil(0.5 * 1.25 * det_halfdiag), max_size]))\n",
    "\n",
    "#     pline = {\n",
    "#         \"kernel\": \"square\",\n",
    "#         \"pixfrac\": 1.0,\n",
    "#         \"pixscale\": 0.06,\n",
    "#         \"size\": int(np.clip(2*est_beam_size*0.06, a_min=3, a_max=30)),\n",
    "#         \"wcs\": None,\n",
    "#     }\n",
    "#     args = auto_script.generate_fit_params(\n",
    "#         pline=pline,\n",
    "#         field_root=root_name,\n",
    "#         min_sens=0.0,\n",
    "#         min_mask=0.0,\n",
    "#         include_photometry=False,  # set both of these to True to include photometry in fitting\n",
    "#         use_phot_obj=False,\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         if not (photz_dir / \"beams\" / f\"{root_name}_{obj_id:05}.beams.fits\").is_file():\n",
    "        \n",
    "#             print (f\"Fetching beams for {obj_id}...\")\n",
    "#             beams = grp.get_beams(\n",
    "#                 obj_id,\n",
    "#                 size=est_beam_size, \n",
    "#                 min_mask=0,\n",
    "#                 min_sens=0,\n",
    "#                 beam_id=\"A\",\n",
    "#             )\n",
    "#             mb = multifit.MultiBeam(\n",
    "#                 beams, fcontam=0.2, min_sens=0.0, min_mask=0, group_name=root_name\n",
    "#             )\n",
    "#             mb.write_master_fits()\n",
    "\n",
    "#             print (f\"Saved beams for {obj_id}.\")\n",
    "#         else:\n",
    "#             shutil.copy2(photz_dir / \"beams\" / f\"{root_name}_{obj_id:05}.beams.fits\", f\"{root_name}_{obj_id:05}.beams.fits\")\n",
    "\n",
    "#         print(f\"Fitting {obj_id}...\")\n",
    "\n",
    "#         if np.isfinite(obj_z):\n",
    "#             zr = [obj_z * 0.9, obj_z * 1.1]\n",
    "#         else:\n",
    "#             zr = [0, 5.0]\n",
    "\n",
    "#         _ = fitting.run_all_parallel(\n",
    "#             obj_id,\n",
    "#             zr=zr,\n",
    "#             dz=[0.01, 0.001],\n",
    "#             verbose=True,\n",
    "#             get_output_data=True,\n",
    "#             skip_complete=False,\n",
    "#             save_figures=True,\n",
    "#         )\n",
    "#         print(\"Fit complete, output saved.\")\n",
    "#         for filetype in [\"beams\", \"full\", \"1D\", \"row\", \"line\", \"log_par\", \"stack\"]:\n",
    "#             [\n",
    "#                 p.rename(photz_dir / filetype / p.name)\n",
    "#                 for p in Path.cwd().glob(f\"*{obj_id}.*{filetype}*\")\n",
    "#             ]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Extraction failed for {obj_id}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12_GLASS_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
